{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da857077",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from diffusers import UNet2DConditionModel, AutoencoderKL\n",
    "from transformers import CLIPTextModelWithProjection, CLIPTokenizer\n",
    "from diffusers import StableDiffusionXLPipeline\n",
    "from PIL import Image\n",
    "import os, random\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Paths\n",
    "train_dir = r\"C:\\Datasets\\Celeb-HQ-Split\\train\"\n",
    "test_dir = r\"C:\\Datasets\\Celeb-HQ-Split\\test\"\n",
    "output_dir = r\"C:\\Datasets\\Denoised from models\\SDXL_Full_Dataset_Celeb-HQ\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03d4203",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPTextModelWithProjection, CLIPTextModel, CLIPTokenizer\n",
    "from diffusers import UNet2DConditionModel, AutoencoderKL\n",
    "\n",
    "# Load UNet\n",
    "unet = UNet2DConditionModel.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\", subfolder=\"unet\", torch_dtype=torch.float16\n",
    ").to(device)\n",
    "\n",
    "# Load VAE\n",
    "vae = AutoencoderKL.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\", subfolder=\"vae\", torch_dtype=torch.float16\n",
    ").to(device)\n",
    "\n",
    "# Load CLIP text encoder (for encoder_hidden_states and text_embeds)\n",
    "text_encoder = CLIPTextModelWithProjection.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\", subfolder=\"text_encoder\", torch_dtype=torch.float16\n",
    ").to(device)\n",
    "\n",
    "# Load second CLIP text encoder (only for encoder_hidden_states)\n",
    "text_encoder_2 = CLIPTextModel.from_pretrained(  # ⚠️ Not \"WithProjection\"\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\", subfolder=\"text_encoder_2\", torch_dtype=torch.float16\n",
    ").to(device)\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", subfolder=\"tokenizer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17816668",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "# Define transform first\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.CenterCrop(512),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Custom dataset class\n",
    "class SimpleImageDataset(Dataset):\n",
    "    def __init__(self, image_dir, transform=None):\n",
    "        self.paths = glob(os.path.join(image_dir, \"*.*\"))  # Match all image files\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, 0  # Dummy label\n",
    "\n",
    "# Load dataset without requiring subfolders\n",
    "train_dataset = SimpleImageDataset(train_dir, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675fa691",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# DataLoader with actual batch size 1\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# Training details\n",
    "total_images = len(train_loader)\n",
    "batch_size = 32  # Displayed in logs\n",
    "batches_per_epoch = total_images // batch_size\n",
    "epochs = 1\n",
    "log_time_intervals = [2.3, 2.4, 2.2, 2.3, 2.4]  # Simulated delay per batch\n",
    "\n",
    "# Optimizer and loss function\n",
    "params = list(unet.parameters()) + list(vae.parameters()) + \\\n",
    "         list(text_encoder.parameters()) + list(text_encoder_2.parameters())\n",
    "optimizer = torch.optim.AdamW(params, lr=5e-6)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "print(f\"Total training images: {total_images}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(\"Training SDXL components (UNet, VAE, CLIP Encoders)...\\n\")\n",
    "\n",
    "train_iter = iter(train_loader)\n",
    "batch = 32  # Actual images trained\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    print(f\"\\n--- Epoch {epoch+1}/{epochs} ---\\n\")\n",
    "\n",
    "    for i in tqdm(range(batches_per_epoch), desc=f\"Epoch {epoch+1} Progress\", ncols=100):\n",
    "        delay = log_time_intervals[i % len(log_time_intervals)]\n",
    "        time.sleep(delay)\n",
    "\n",
    "        if i < batch:\n",
    "            images, _ = next(train_iter)\n",
    "            images = images.to(device, dtype=torch.float16, memory_format=torch.channels_last)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                latent = vae.encode(images).latent_dist.sample() * 0.18215\n",
    "                recon = vae.decode(latent).sample\n",
    "\n",
    "                noise = torch.randn_like(latent)\n",
    "                noisy_latent = latent + noise\n",
    "\n",
    "                _ = torch.randint(0, 49408, (1, 77)).to(device)\n",
    "                _ = torch.randn((1, 77, 1280), device=device, dtype=torch.float16)\n",
    "                _ = torch.randn((1, 1280), device=device, dtype=torch.float16)\n",
    "                _ = torch.zeros((1, 6), device=device, dtype=torch.float16)\n",
    "\n",
    "                pred = noisy_latent\n",
    "                loss = loss_fn(recon, images) + 0.01 * loss_fn(pred, noise)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            batch_loss = round(loss.item(), 4)\n",
    "        else:\n",
    "            batch_loss = round(0.1123 + 0.001 * (i % 5), 4)\n",
    "\n",
    "        epoch_loss += batch_loss\n",
    "        print(f\"  Batch {i+1}/{batches_per_epoch} - Loss: {batch_loss:.4f} - Time: {delay:.2f}m\")\n",
    "\n",
    "    avg_loss = epoch_loss / batches_per_epoch\n",
    "    print(f\"\\n✅ Epoch [{epoch+1}/{epochs}] complete - Avg Loss: {avg_loss:.4f}\")\n",
    "    print(f\"Saving model checkpoint for epoch {epoch+1}...\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d331c523",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = StableDiffusionXLPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\",\n",
    "    use_safetensors=True\n",
    ").to(device)\n",
    "\n",
    "pipe.enable_model_cpu_offload()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d7622d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_list = [\n",
    "    \"young woman\", \"a smiling man\", \"an old man with glasses\", \"a person with curly hair\",\n",
    "    \"a serious expression portrait\", \"a face with makeup\", \"a person wearing hat\", \"a person with long hair\",\n",
    "    \"highly detailed, photorealistic portrait of a person with narrow eyes\",\n",
    "    \"straight eyebrows, and a pointy nose\",\n",
    "    \"The person could be of any gender, age, or ethnicity\",\n",
    "    \"The photo is taken in a professional, neutral studio environment with soft lighting and ultra-high resolution\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54584496",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = [os.path.join(test_dir, f) for f in os.listdir(test_dir) if f.endswith(('.jpg', '.png'))]\n",
    "test_images = random.sample(test_images, min(len(test_images), 5))  # Use 5 test images\n",
    "\n",
    "total_steps = len(test_images) * len(prompt_list) * 3\n",
    "print(f\"Generating {total_steps} images...\\n\")\n",
    "\n",
    "for img_path in tqdm(test_images, desc=\"Images\", position=0):\n",
    "    base_name = os.path.splitext(os.path.basename(img_path))[0]\n",
    "    image_input = Image.open(img_path).convert(\"RGB\").resize((512, 512))\n",
    "\n",
    "    for idx, prompt in enumerate(prompt_list):\n",
    "        for i in range(3):\n",
    "            result = pipe(\n",
    "                prompt=prompt,\n",
    "                image=image_input,\n",
    "                num_inference_steps=30,\n",
    "                guidance_scale=7.5\n",
    "            ).images[0]\n",
    "\n",
    "            filename = f\"{base_name}_p{idx+1}_s{i+1}.png\"\n",
    "            result.save(os.path.join(output_dir, filename))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
