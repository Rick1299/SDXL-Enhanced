{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098efa18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from PIL import Image\n",
    "from diffusers import DDPMScheduler\n",
    "from transformers import T5Tokenizer, T5EncoderModel\n",
    "from pytorch_wavelets import DWTForward, DWTInverse\n",
    "from torch import optim\n",
    "from datetime import datetime\n",
    "from diffusers import StableDiffusionXLImg2ImgPipeline\n",
    "\n",
    "\n",
    "# Configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_dir = r'C:\\Datasets\\Celeb-HQ-Split\\train'\n",
    "test_dir = r'C:\\Datasets\\Celeb-HQ-Split\\test'\n",
    "output_dir = r'C:\\Datasets\\Denoised from models\\SDXL_Enhanced_Celeb-HQ'\n",
    "checkpoint_dir = r\"C:\\Datasets\\Checkpoints\\sdxl_t5_litevae_txt\"\n",
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = (1024, 1024)\n",
    "NUM_SAMPLES_PER_IMAGE = 1\n",
    "STRENGTH = 0.9\n",
    "GUIDANCE_SCALE = 10\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d79c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_with_padding(image, target_size=(1024, 1024)):\n",
    "    old_size = image.size\n",
    "    ratio = min(target_size[0] / old_size[0], target_size[1] / old_size[1])\n",
    "    new_size = tuple([int(x * ratio) for x in old_size])\n",
    "    image = image.resize(new_size, Image.LANCZOS)\n",
    "    new_image = Image.new(\"RGB\", target_size, (0, 0, 0))\n",
    "    new_image.paste(image, ((target_size[0] - new_size[0]) // 2, (target_size[1] - new_size[1]) // 2))\n",
    "    return new_image\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = [\n",
    "            os.path.join(root_dir, img_name)\n",
    "            for img_name in os.listdir(root_dir)\n",
    "            if img_name.lower().endswith(('.png', '.jpg', '.jpeg'))\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        image = resize_with_padding(image, IMG_SIZE)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, img_path\n",
    "\n",
    "# Transforms and loaders\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda t: (t * 2) - 1)\n",
    "])\n",
    "\n",
    "train_dataset = CustomImageDataset(train_dir, transform=data_transforms)\n",
    "test_dataset = CustomImageDataset(test_dir, transform=data_transforms)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a251e56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LiteVAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dwt = DWTForward(J=1, mode='zero', wave='haar')\n",
    "        self.iwt = DWTInverse(mode='zero', wave='haar')\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 4, 2, 1), nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1), nn.ReLU(),\n",
    "            nn.Conv2d(128, 4, 3, 1, 1)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(4, 128, 4, 2, 1), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 3, 3, 1, 1), nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        yl, _ = self.dwt(x)\n",
    "        return self.encoder(yl)\n",
    "\n",
    "    def decode(self, z):\n",
    "        decoded = self.decoder(z)\n",
    "        return self.iwt((decoded, [None]))\n",
    "\n",
    "class TinyUNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(4, 32, 3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU()\n",
    "        )\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, 3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(64, 4, 3, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t, encoder_hidden_states=None):\n",
    "        return self.block2(self.block1(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab49508",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = LiteVAE().to(device)\n",
    "unet = TinyUNet().to(device)\n",
    "text_encoder = T5EncoderModel.from_pretrained(\"t5-base\").to(device)\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "scheduler = DDPMScheduler.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", subfolder=\"scheduler\")\n",
    "optimizer = optim.AdamW(unet.parameters(), lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe096404",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = [\n",
    "            os.path.join(root_dir, img_name)\n",
    "            for img_name in os.listdir(root_dir)\n",
    "            if img_name.lower().endswith(('.png', '.jpg', '.jpeg'))\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        image = resize_with_padding(image, (1024, 1024))\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, img_path\n",
    "\n",
    "# Define transformations\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda t: (t * 2) - 1)\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "def load_dataset():\n",
    "    train_dataset = CustomImageDataset(train_dir, transform=data_transforms)\n",
    "    test_dataset = CustomImageDataset(test_dir, transform=data_transforms)\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "train_dataset, test_dataset = load_dataset()\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff35c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_t5_litevae_model(dataloader, epochs=2, sleep_per_step=0):\n",
    "    print(\"üöÄ Training SDXL with T5 + LiteVAE started...\\n\", flush=True)\n",
    "    start_time = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        unet.train()\n",
    "        total_loss = 0\n",
    "        epoch_start = time.time()\n",
    "        print(f\"üìò Epoch {epoch + 1}/{epochs}...\\n\", flush=True)\n",
    "\n",
    "        for step, (images, img_paths) in enumerate(dataloader):\n",
    "            image_name = os.path.basename(img_paths[0])\n",
    "            print(f\"üñºÔ∏è  Training on image {step + 1}/{len(dataloader)}: {image_name}\", flush=True)\n",
    "\n",
    "            images = images.to(device)\n",
    "            with torch.no_grad():\n",
    "                latents = vae.encode(images)\n",
    "\n",
    "            noise = torch.randn_like(latents)\n",
    "            timesteps = torch.randint(0, 1000, (latents.shape[0],), device=device).long()\n",
    "            noisy_latents = scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "            prompts = [\n",
    "    \"young woman\", \"a smiling man\", \"an old man with glasses\", \"a person with curly hair\",\n",
    "    \"a serious expression portrait\", \"a face with makeup\", \"a person wearing hat\", \"a person with long hair\",\n",
    "    \"highly detailed, photorealistic portrait of a person with narrow eyes\",\n",
    "    \"straight eyebrows, and a pointy nose\",\n",
    "    \"The person could be of any gender, age, or ethnicity\",\n",
    "    \"The photo is taken in a professional, neutral studio environment with soft lighting and ultra-high resolution\"\n",
    "] * images.size(0)\n",
    "            inputs = tokenizer(prompts, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=77).to(device)\n",
    "            with torch.no_grad():\n",
    "                encoder_hidden_states = text_encoder(**inputs).last_hidden_state\n",
    "\n",
    "            noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states)\n",
    "            loss = F.mse_loss(noise_pred, noise)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            torch.cuda.synchronize()\n",
    "            time.sleep(sleep_per_step)\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        print(f\"\\n‚úÖ Epoch {epoch+1} complete | Avg Loss: {avg_loss:.4f} | Duration: {epoch_time:.2f} sec\\n\", flush=True)\n",
    "        torch.save(unet.state_dict(), os.path.join(checkpoint_dir, f\"unet_epoch_{epoch+1}.pt\"))\n",
    "        torch.save(vae.state_dict(), os.path.join(checkpoint_dir, f\"vae_epoch_{epoch+1}.pt\"))\n",
    "        with open(os.path.join(checkpoint_dir, f\"epoch_{epoch+1}_log.txt\"), \"w\") as f:\n",
    "            f.write(f\"Epoch: {epoch+1}\\nAverage Loss: {avg_loss:.4f}\\nTimestamp: {timestamp}\\nEpoch Duration: {epoch_time:.2f} sec\\n\")\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"üéØ Training finished in {(total_time/60):.2f} minutes.\")\n",
    "    print(f\"üìÅ Logs saved to: {checkpoint_dir}\", flush=True)\n",
    "    \n",
    "train_t5_litevae_model(train_loader, epochs=1, sleep_per_step=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d99bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_and_save_images_with_custom_model(dataloader, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    model = StableDiffusionXLImg2ImgPipeline.from_pretrained(\n",
    "        \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "        torch_dtype=torch.float16,\n",
    "        variant=\"fp16\",\n",
    "        use_safetensors=True\n",
    "    ).to(device)\n",
    "    model.enable_attention_slicing()\n",
    "\n",
    "    prompts = [\n",
    "        \"young woman\", \"a smiling man\", \"an old man with glasses\", \"a person with curly hair\",\n",
    "        \"a serious expression portrait\", \"a face with makeup\", \"a person wearing hat\", \"a person with long hair\",\n",
    "        \"highly detailed, photorealistic portrait of a person with narrow eyes\",\n",
    "        \"straight eyebrows, and a pointy nose\",\n",
    "        \"The person could be of any gender, age, or ethnicity\",\n",
    "        \"The photo is taken in a professional, neutral studio environment with soft lighting and ultra-high resolution\"\n",
    "    ]\n",
    "\n",
    "    for batch_idx, (images, img_paths) in enumerate(dataloader):\n",
    "        for i in range(NUM_SAMPLES_PER_IMAGE):\n",
    "            img = images[0].unsqueeze(0).to(device)\n",
    "            img_pil = transforms.ToPILImage()(img.squeeze(0).cpu().detach())\n",
    "\n",
    "            prompt = prompts[i % len(prompts)]\n",
    "\n",
    "            result = model(\n",
    "                prompt=prompt,\n",
    "                image=img_pil,\n",
    "                strength=STRENGTH,\n",
    "                guidance_scale=GUIDANCE_SCALE\n",
    "            )\n",
    "            output_image = result.images[0]\n",
    "\n",
    "            filename = os.path.splitext(os.path.basename(img_paths[0]))[0]\n",
    "            save_path = os.path.join(output_dir, f\"{filename}_sample_{i}.png\")\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            output_image.save(save_path)\n",
    "\n",
    "    print(f\"üñºÔ∏è  All generated images saved to: {output_dir}\")\n",
    "\n",
    "# Call the function\n",
    "generate_and_save_images_with_custom_model(test_loader, output_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
